{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true,
    "ExecuteTime": {
     "end_time": "2023-08-09T15:46:40.088493700Z",
     "start_time": "2023-08-09T15:46:30.163921300Z"
    }
   },
   "outputs": [],
   "source": [
    "## Importing Libraries\n",
    "import numpy as np\n",
    "import cv2\n",
    "cap= cv2.VideoCapture(0)\n",
    "from mtcnn import MTCNN\n",
    "\n",
    "## Loading the model\n",
    "detector=MTCNN()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "outputs": [],
   "source": [
    "# Function to detect faces and facial keypoints in an image and display the results\n",
    "def show_faces_and_alignment(img1):\n",
    "    # Detect faces in the input image\n",
    "    result1=detector.detect_faces(img1)\n",
    "    print(result1)\n",
    "    # Loop through the detected faces and draw bounding boxes and keypoints\n",
    "    for i in range(len(result1)):\n",
    "      if(result1[i]['confidence']>0.9):\n",
    "        # Get the coordinates of the bounding box\n",
    "        x1,y1,w1,h1=result1[i]['box']\n",
    "        # Get the coordinates of the facial keypoints\n",
    "        lex,ley=result1[i]['keypoints']['left_eye']\n",
    "        rex,rey=result1[i]['keypoints']['right_eye']\n",
    "        nosex,nosey=result1[i]['keypoints']['nose']\n",
    "        mlx,mly=result1[i]['keypoints']['mouth_left']\n",
    "        mrx,mry=result1[i]['keypoints']['mouth_right']\n",
    "        # Draw bounding box and keypoints on the image\n",
    "        cv2.rectangle(img1,(x1,y1),(x1+w1,y1+h1),(0,255,0),1)\n",
    "        cv2.circle(img1,(lex,ley),radius=1,color=(255,0,0),thickness=1)\n",
    "        cv2.circle(img1,(rex,rey),radius=1,color=(255,0,0),thickness=1)\n",
    "        cv2.circle(img1,(nosex,nosey),radius=1,color=(255,0,0),thickness=1)\n",
    "        cv2.circle(img1,(mlx,mly),radius=1,color=(255,0,0),thickness=1)\n",
    "        cv2.circle(img1,(mrx,mry),radius=1,color=(255,0,0),thickness=1)\n",
    "    # Display the image with bounding boxes and keypoints\n",
    "    cv2.imshow('window',img1)\n",
    "    cv2.waitKey()\n"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-08-09T15:46:40.102648400Z",
     "start_time": "2023-08-09T15:46:40.092494900Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 0s 308ms/step\n",
      "1/1 [==============================] - 0s 95ms/step\n",
      "1/1 [==============================] - 0s 23ms/step\n",
      "1/1 [==============================] - 0s 21ms/step\n",
      "1/1 [==============================] - 0s 20ms/step\n",
      "1/1 [==============================] - 0s 22ms/step\n",
      "1/1 [==============================] - 0s 28ms/step\n",
      "1/1 [==============================] - 0s 29ms/step\n",
      "1/1 [==============================] - 0s 142ms/step\n",
      "1/1 [==============================] - 0s 110ms/step\n",
      "[{'box': [220, 71, 75, 96], 'confidence': 0.999087929725647, 'keypoints': {'left_eye': (238, 107), 'right_eye': (272, 105), 'nose': (255, 129), 'mouth_left': (240, 142), 'mouth_right': (274, 141)}}]\n"
     ]
    }
   ],
   "source": [
    "# Read and resize the input image, then call the function to detect faces and facial keypoints\n",
    "img=cv2.imread(\"images/image1.jpg\")\n",
    "img=cv2.resize(img,(400,300))\n",
    "\n",
    "show_faces_and_alignment(img)\n"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-08-09T15:46:45.201503100Z",
     "start_time": "2023-08-09T15:46:40.102648400Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 0s 29ms/step\n",
      "1/1 [==============================] - 0s 30ms/step\n",
      "1/1 [==============================] - 0s 27ms/step\n",
      "1/1 [==============================] - 0s 27ms/step\n",
      "1/1 [==============================] - 0s 32ms/step\n",
      "1/1 [==============================] - 0s 20ms/step\n",
      "1/1 [==============================] - 0s 25ms/step\n",
      "1/1 [==============================] - 0s 20ms/step\n",
      "1/1 [==============================] - 0s 20ms/step\n",
      "1/1 [==============================] - 0s 22ms/step\n",
      "[{'box': [139, 78, 120, 123], 'confidence': 0.9683423042297363, 'keypoints': {'left_eye': (177, 122), 'right_eye': (234, 120), 'nose': (208, 144), 'mouth_left': (181, 170), 'mouth_right': (232, 169)}}]\n"
     ]
    }
   ],
   "source": [
    "# Read and resize the input image, then call the function to detect faces and facial keypoints\n",
    "img1=cv2.imread(\"images/image2.jpg\")\n",
    "img1=cv2.resize(img1,(400,300))\n",
    "\n",
    "show_faces_and_alignment(img1)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-08-09T15:46:47.162509200Z",
     "start_time": "2023-08-09T15:46:45.191344200Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 0s 77ms/step\n",
      "1/1 [==============================] - 0s 66ms/step\n",
      "1/1 [==============================] - 0s 38ms/step\n",
      "1/1 [==============================] - 0s 16ms/step\n",
      "1/1 [==============================] - 0s 31ms/step\n",
      "1/1 [==============================] - 0s 22ms/step\n",
      "1/1 [==============================] - 0s 31ms/step\n",
      "1/1 [==============================] - 0s 16ms/step\n",
      "1/1 [==============================] - 0s 25ms/step\n",
      "1/1 [==============================] - 0s 16ms/step\n",
      "4/4 [==============================] - 0s 5ms/step\n",
      "2/2 [==============================] - 0s 9ms/step\n",
      "[{'box': [78, 319, 48, 52], 'confidence': 0.9999462366104126, 'keypoints': {'left_eye': (92, 339), 'right_eye': (115, 339), 'nose': (103, 350), 'mouth_left': (92, 358), 'mouth_right': (114, 358)}}, {'box': [265, 176, 46, 54], 'confidence': 0.9998582601547241, 'keypoints': {'left_eye': (280, 196), 'right_eye': (302, 196), 'nose': (292, 206), 'mouth_left': (283, 217), 'mouth_right': (300, 218)}}, {'box': [290, 462, 46, 52], 'confidence': 0.9998267292976379, 'keypoints': {'left_eye': (307, 479), 'right_eye': (328, 484), 'nose': (317, 489), 'mouth_left': (303, 498), 'mouth_right': (322, 502)}}, {'box': [656, 321, 46, 49], 'confidence': 0.9997281432151794, 'keypoints': {'left_eye': (665, 341), 'right_eye': (685, 339), 'nose': (672, 352), 'mouth_left': (669, 362), 'mouth_right': (686, 360)}}, {'box': [475, 467, 45, 49], 'confidence': 0.9996367692947388, 'keypoints': {'left_eye': (488, 486), 'right_eye': (509, 485), 'nose': (498, 495), 'mouth_left': (490, 506), 'mouth_right': (508, 506)}}, {'box': [694, 20, 44, 49], 'confidence': 0.9994534850120544, 'keypoints': {'left_eye': (709, 37), 'right_eye': (729, 41), 'nose': (717, 45), 'mouth_left': (704, 55), 'mouth_right': (723, 58)}}, {'box': [93, 467, 46, 55], 'confidence': 0.9988341927528381, 'keypoints': {'left_eye': (111, 486), 'right_eye': (132, 490), 'nose': (122, 501), 'mouth_left': (107, 508), 'mouth_right': (126, 512)}}, {'box': [256, 320, 44, 51], 'confidence': 0.9986261129379272, 'keypoints': {'left_eye': (269, 340), 'right_eye': (290, 339), 'nose': (281, 350), 'mouth_left': (273, 360), 'mouth_right': (291, 359)}}, {'box': [472, 320, 43, 48], 'confidence': 0.9985951781272888, 'keypoints': {'left_eye': (486, 340), 'right_eye': (507, 340), 'nose': (496, 349), 'mouth_left': (487, 359), 'mouth_right': (505, 359)}}, {'box': [95, 24, 43, 49], 'confidence': 0.9982684850692749, 'keypoints': {'left_eye': (110, 41), 'right_eye': (130, 44), 'nose': (120, 51), 'mouth_left': (109, 60), 'mouth_right': (125, 63)}}, {'box': [277, 28, 43, 51], 'confidence': 0.9973681569099426, 'keypoints': {'left_eye': (289, 48), 'right_eye': (309, 48), 'nose': (298, 58), 'mouth_left': (290, 68), 'mouth_right': (308, 68)}}, {'box': [452, 173, 44, 50], 'confidence': 0.9893869161605835, 'keypoints': {'left_eye': (462, 193), 'right_eye': (483, 190), 'nose': (472, 201), 'mouth_left': (468, 213), 'mouth_right': (484, 211)}}, {'box': [31, 172, 44, 49], 'confidence': 0.9650214314460754, 'keypoints': {'left_eye': (40, 191), 'right_eye': (61, 186), 'nose': (51, 199), 'mouth_left': (48, 211), 'mouth_right': (65, 207)}}, {'box': [675, 174, 47, 54], 'confidence': 0.8873327970504761, 'keypoints': {'left_eye': (693, 192), 'right_eye': (715, 196), 'nose': (703, 204), 'mouth_left': (689, 214), 'mouth_right': (707, 218)}}]\n"
     ]
    }
   ],
   "source": [
    "# Read and resize the input image, then call the function to detect faces and facial keypoints\n",
    "img2=cv2.imread(\"images/multiple1.jpg\")\n",
    "img2=cv2.resize(img2,(800,600))\n",
    "\n",
    "show_faces_and_alignment(img2)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-08-09T15:46:50.506239300Z",
     "start_time": "2023-08-09T15:46:47.148398500Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 0s 63ms/step\n",
      "1/1 [==============================] - 0s 59ms/step\n",
      "1/1 [==============================] - 0s 42ms/step\n",
      "1/1 [==============================] - 0s 23ms/step\n",
      "1/1 [==============================] - 0s 29ms/step\n",
      "1/1 [==============================] - 0s 29ms/step\n",
      "1/1 [==============================] - 0s 22ms/step\n",
      "1/1 [==============================] - 0s 22ms/step\n",
      "1/1 [==============================] - 0s 21ms/step\n",
      "1/1 [==============================] - 0s 26ms/step\n",
      "4/4 [==============================] - 0s 5ms/step\n",
      "2/2 [==============================] - 0s 11ms/step\n",
      "[{'box': [621, 453, 78, 96], 'confidence': 0.9999644756317139, 'keypoints': {'left_eye': (636, 491), 'right_eye': (670, 491), 'nose': (647, 512), 'mouth_left': (637, 527), 'mouth_right': (669, 528)}}, {'box': [634, 250, 81, 96], 'confidence': 0.9999455213546753, 'keypoints': {'left_eye': (662, 286), 'right_eye': (701, 289), 'nose': (683, 307), 'mouth_left': (659, 323), 'mouth_right': (696, 326)}}, {'box': [359, 459, 77, 92], 'confidence': 0.9998347759246826, 'keypoints': {'left_eye': (380, 492), 'right_eye': (416, 493), 'nose': (397, 515), 'mouth_left': (382, 528), 'mouth_right': (414, 529)}}, {'box': [86, 248, 93, 104], 'confidence': 0.9997478127479553, 'keypoints': {'left_eye': (112, 287), 'right_eye': (155, 288), 'nose': (134, 308), 'mouth_left': (109, 320), 'mouth_right': (155, 322)}}, {'box': [360, 45, 81, 101], 'confidence': 0.9997077584266663, 'keypoints': {'left_eye': (381, 84), 'right_eye': (419, 84), 'nose': (399, 107), 'mouth_left': (385, 123), 'mouth_right': (417, 123)}}, {'box': [86, 41, 87, 102], 'confidence': 0.9995973706245422, 'keypoints': {'left_eye': (108, 82), 'right_eye': (149, 80), 'nose': (129, 98), 'mouth_left': (112, 118), 'mouth_right': (149, 118)}}, {'box': [85, 448, 89, 103], 'confidence': 0.9994474053382874, 'keypoints': {'left_eye': (109, 485), 'right_eye': (151, 488), 'nose': (129, 505), 'mouth_left': (107, 524), 'mouth_right': (146, 526)}}, {'box': [353, 237, 92, 112], 'confidence': 0.9989973902702332, 'keypoints': {'left_eye': (379, 278), 'right_eye': (422, 276), 'nose': (401, 289), 'mouth_left': (385, 320), 'mouth_right': (415, 319)}}, {'box': [625, 42, 87, 98], 'confidence': 0.9862121343612671, 'keypoints': {'left_eye': (645, 81), 'right_eye': (685, 75), 'nose': (666, 96), 'mouth_left': (652, 117), 'mouth_right': (691, 112)}}]\n"
     ]
    }
   ],
   "source": [
    "# Read and resize the input image, then call the function to detect faces and facial keypoints\n",
    "img3=cv2.imread(\"images/mul2.jpg\")\n",
    "img3=cv2.resize(img3,(800,600))\n",
    "\n",
    "show_faces_and_alignment(img3)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-08-09T15:46:53.897415600Z",
     "start_time": "2023-08-09T15:46:50.412083900Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "outputs": [],
   "source": [
    "## for input and output of same shape\n",
    "## n+2p-f+1=n  :: p=padding, f=window size\n",
    "## p = (f-1)/2\n",
    "\n",
    "## function to blur the image\n",
    "def blur_the_image(img):\n",
    "    x=img.shape[0]\n",
    "    y=img.shape[1]\n",
    "    kernel_size=50\n",
    "    p=(kernel_size-1)\n",
    "    padded_img= np.zeros((x+p,y+p,3), dtype=np.float32)\n",
    "    p=p//2\n",
    "    padded_img[p:x+p,p:y+p,:]=img\n",
    "\n",
    "    window=np.ones((kernel_size, kernel_size), dtype=np.float32) / (kernel_size**2)\n",
    "    # window = np.array([[1, 2, 1],\n",
    "    #                       [2, 4, 2],\n",
    "    #                       [1, 2, 1]], dtype=np.float32) / 9.0\n",
    "    for i in range(x):\n",
    "        for j in range(y):\n",
    "            for k in range(3):\n",
    "                img[i,j,k]= np.sum((padded_img[i:i+kernel_size,j:j+kernel_size,k]*window))\n",
    "\n",
    "    return img\n"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-08-09T15:46:53.903549600Z",
     "start_time": "2023-08-09T15:46:53.896349Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 0s 31ms/step\n",
      "1/1 [==============================] - 0s 31ms/step\n",
      "1/1 [==============================] - 0s 31ms/step\n",
      "1/1 [==============================] - 0s 17ms/step\n",
      "1/1 [==============================] - 0s 31ms/step\n",
      "1/1 [==============================] - 0s 25ms/step\n",
      "1/1 [==============================] - 0s 16ms/step\n",
      "1/1 [==============================] - 0s 16ms/step\n",
      "1/1 [==============================] - 0s 16ms/step\n",
      "1/1 [==============================] - 0s 31ms/step\n",
      "2/2 [==============================] - 0s 0s/step\n",
      "1/1 [==============================] - 0s 16ms/step\n"
     ]
    },
    {
     "data": {
      "text/plain": "-1"
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "imgx=cv2.imread(\"images/image1.jpg\")\n",
    "imgx=cv2.resize(imgx,(800,600))\n",
    "result1=detector.detect_faces(imgx)\n",
    "xx,yy,ww,hh=result1[0]['box']\n",
    "imgx[yy:yy+hh,xx:xx+ww]=blur_the_image(imgx[yy:yy+hh,xx:xx+ww])\n",
    "\n",
    "cv2.imshow('window',imgx)\n",
    "cv2.waitKey()"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-08-09T15:46:58.451642400Z",
     "start_time": "2023-08-09T15:46:53.907626200Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 0s 31ms/step\n",
      "1/1 [==============================] - 0s 32ms/step\n",
      "1/1 [==============================] - 0s 16ms/step\n",
      "1/1 [==============================] - 0s 16ms/step\n",
      "1/1 [==============================] - 0s 16ms/step\n",
      "1/1 [==============================] - 0s 31ms/step\n",
      "1/1 [==============================] - 0s 25ms/step\n",
      "1/1 [==============================] - 0s 32ms/step\n",
      "1/1 [==============================] - 0s 16ms/step\n",
      "1/1 [==============================] - 0s 32ms/step\n",
      "3/3 [==============================] - 0s 8ms/step\n",
      "1/1 [==============================] - 0s 31ms/step\n",
      "1/1 [==============================] - 0s 31ms/step\n",
      "1/1 [==============================] - 0s 31ms/step\n",
      "1/1 [==============================] - 0s 32ms/step\n",
      "1/1 [==============================] - 0s 31ms/step\n",
      "1/1 [==============================] - 0s 16ms/step\n",
      "1/1 [==============================] - 0s 16ms/step\n",
      "1/1 [==============================] - 0s 31ms/step\n",
      "1/1 [==============================] - 0s 32ms/step\n",
      "1/1 [==============================] - 0s 16ms/step\n",
      "1/1 [==============================] - 0s 21ms/step\n",
      "2/2 [==============================] - 0s 0s/step\n",
      "1/1 [==============================] - 0s 31ms/step\n",
      "1/1 [==============================] - 0s 31ms/step\n",
      "1/1 [==============================] - 0s 33ms/step\n",
      "1/1 [==============================] - 0s 31ms/step\n",
      "1/1 [==============================] - 0s 16ms/step\n",
      "1/1 [==============================] - 0s 32ms/step\n",
      "1/1 [==============================] - 0s 16ms/step\n",
      "1/1 [==============================] - 0s 22ms/step\n",
      "1/1 [==============================] - 0s 31ms/step\n",
      "1/1 [==============================] - 0s 16ms/step\n",
      "1/1 [==============================] - 0s 16ms/step\n",
      "1/1 [==============================] - 0s 14ms/step\n",
      "1/1 [==============================] - 0s 16ms/step\n",
      "1/1 [==============================] - 0s 31ms/step\n",
      "1/1 [==============================] - 0s 16ms/step\n",
      "1/1 [==============================] - 0s 31ms/step\n",
      "1/1 [==============================] - 0s 31ms/step\n",
      "1/1 [==============================] - 0s 23ms/step\n",
      "1/1 [==============================] - 0s 16ms/step\n",
      "1/1 [==============================] - 0s 16ms/step\n",
      "1/1 [==============================] - 0s 16ms/step\n",
      "1/1 [==============================] - 0s 32ms/step\n",
      "1/1 [==============================] - 0s 16ms/step\n",
      "2/2 [==============================] - 0s 0s/step\n",
      "1/1 [==============================] - 0s 16ms/step\n",
      "1/1 [==============================] - 0s 31ms/step\n",
      "1/1 [==============================] - 0s 32ms/step\n",
      "1/1 [==============================] - 0s 16ms/step\n",
      "1/1 [==============================] - 0s 31ms/step\n",
      "1/1 [==============================] - 0s 31ms/step\n",
      "1/1 [==============================] - 0s 16ms/step\n",
      "1/1 [==============================] - 0s 16ms/step\n",
      "1/1 [==============================] - 0s 31ms/step\n",
      "1/1 [==============================] - 0s 31ms/step\n",
      "1/1 [==============================] - 0s 18ms/step\n",
      "2/2 [==============================] - 0s 0s/step\n",
      "1/1 [==============================] - 0s 31ms/step\n",
      "1/1 [==============================] - 0s 31ms/step\n",
      "1/1 [==============================] - 0s 16ms/step\n",
      "1/1 [==============================] - 0s 32ms/step\n",
      "1/1 [==============================] - 0s 31ms/step\n",
      "1/1 [==============================] - 0s 13ms/step\n",
      "1/1 [==============================] - 0s 16ms/step\n",
      "1/1 [==============================] - 0s 16ms/step\n",
      "1/1 [==============================] - 0s 16ms/step\n",
      "1/1 [==============================] - 0s 16ms/step\n",
      "1/1 [==============================] - 0s 16ms/step\n",
      "3/3 [==============================] - 0s 8ms/step\n",
      "1/1 [==============================] - 0s 19ms/step\n",
      "1/1 [==============================] - 0s 47ms/step\n",
      "1/1 [==============================] - 0s 31ms/step\n",
      "1/1 [==============================] - 0s 29ms/step\n",
      "1/1 [==============================] - 0s 16ms/step\n",
      "1/1 [==============================] - 0s 32ms/step\n",
      "1/1 [==============================] - 0s 31ms/step\n",
      "1/1 [==============================] - 0s 7ms/step\n",
      "1/1 [==============================] - 0s 31ms/step\n",
      "1/1 [==============================] - 0s 16ms/step\n",
      "1/1 [==============================] - 0s 31ms/step\n",
      "2/2 [==============================] - 0s 16ms/step\n",
      "1/1 [==============================] - 0s 28ms/step\n",
      "1/1 [==============================] - 0s 40ms/step\n",
      "1/1 [==============================] - 0s 32ms/step\n",
      "1/1 [==============================] - 0s 16ms/step\n",
      "1/1 [==============================] - 0s 32ms/step\n",
      "1/1 [==============================] - 0s 16ms/step\n",
      "1/1 [==============================] - 0s 16ms/step\n",
      "1/1 [==============================] - 0s 31ms/step\n",
      "1/1 [==============================] - 0s 16ms/step\n",
      "1/1 [==============================] - 0s 16ms/step\n",
      "1/1 [==============================] - 0s 31ms/step\n",
      "2/2 [==============================] - 0s 0s/step\n",
      "1/1 [==============================] - 0s 31ms/step\n",
      "1/1 [==============================] - 0s 31ms/step\n",
      "1/1 [==============================] - 0s 31ms/step\n",
      "1/1 [==============================] - 0s 17ms/step\n",
      "1/1 [==============================] - 0s 16ms/step\n",
      "1/1 [==============================] - 0s 30ms/step\n",
      "1/1 [==============================] - 0s 16ms/step\n",
      "1/1 [==============================] - 0s 16ms/step\n",
      "1/1 [==============================] - 0s 18ms/step\n",
      "1/1 [==============================] - 0s 16ms/step\n",
      "1/1 [==============================] - 0s 30ms/step\n",
      "2/2 [==============================] - 0s 8ms/step\n",
      "1/1 [==============================] - 0s 31ms/step\n",
      "1/1 [==============================] - 0s 47ms/step\n",
      "1/1 [==============================] - 0s 31ms/step\n",
      "1/1 [==============================] - 0s 31ms/step\n",
      "1/1 [==============================] - 0s 16ms/step\n",
      "1/1 [==============================] - 0s 31ms/step\n",
      "1/1 [==============================] - 0s 31ms/step\n",
      "1/1 [==============================] - 0s 31ms/step\n",
      "1/1 [==============================] - 0s 31ms/step\n",
      "1/1 [==============================] - 0s 31ms/step\n",
      "1/1 [==============================] - 0s 31ms/step\n",
      "3/3 [==============================] - 0s 0s/step\n",
      "1/1 [==============================] - 0s 16ms/step\n",
      "1/1 [==============================] - 0s 31ms/step\n",
      "1/1 [==============================] - 0s 32ms/step\n",
      "1/1 [==============================] - 0s 16ms/step\n",
      "1/1 [==============================] - 0s 26ms/step\n",
      "1/1 [==============================] - 0s 16ms/step\n",
      "1/1 [==============================] - 0s 16ms/step\n",
      "1/1 [==============================] - 0s 16ms/step\n",
      "1/1 [==============================] - 0s 16ms/step\n",
      "1/1 [==============================] - 0s 16ms/step\n",
      "1/1 [==============================] - 0s 31ms/step\n",
      "2/2 [==============================] - 0s 3ms/step\n",
      "1/1 [==============================] - 0s 16ms/step\n"
     ]
    }
   ],
   "source": [
    "# Continuously capture frames from the webcam, detect faces, and apply median blur to each detected face\n",
    "while True:\n",
    "    ret, frame=cap.read() # Capture a frame from the webcam\n",
    "    result= detector.detect_faces(frame) # Detect faces in the captured frame\n",
    "\n",
    "    for op in result:\n",
    "        x,y,w,h=op['box']\n",
    "        # cv2.rectangle(frame,(x,y),(x+w,y+h),color=(255,0,0),thickness=2)\n",
    "        face=frame[y:y+h,x:x+w] # Extract the region of interest (face)\n",
    "        # blurred = cv2.medianBlur(face, 99) # Apply median blur to the face region\n",
    "        blurred=blur_the_image(face)\n",
    "        frame[y:y+h,x:x+w]=blurred # Replace the face region with the blurred version\n",
    "    cv2.imshow('window',frame) # Display the frame with blurred faces\n",
    "\n",
    "    if cv2.waitKey(1) & 0xFF==ord('x'):\n",
    "        break\n",
    "\n",
    "cv2.destroyAllWindows()"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-08-09T15:47:24.541801900Z",
     "start_time": "2023-08-09T15:46:58.451642400Z"
    }
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
